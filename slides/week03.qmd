---
title: "Exploratory Data Analysis"
subtitle: "BIOSTAT 620: Introduction to Health Data Science"
author: "Dylan Cable"
format:
  revealjs:
    theme: default
    fig-width: 7
    fig-align: center
engine: knitr
execute: 
  cache: false
---

```{r, include=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE)
```

# Exploratory Data Analysis

Main topic for today

---

## Acknowledgment

This lecture includes some slides originally developed by Meredith Franklin. They have been modified by George G. Vega Yon, Kelly Street, and Dylan Cable.

---

## Exploratory Data Analysis{.smaller}

* Exploratory data analysis is the process of becoming familiar with a dataset
* It should be the first step in your analysis pipeline
* It involves:
  + checking data (import issues, outliers, missing values, data errors)
  + cleaning data
  + summary statistics of key variables (univariate and bivariate)
  + basic plots and graphs

---

## Exploratory Data Analysis{.smaller}

> Since our eyes and brains are not wired to detect patterns in large data tables filled with text and numbers, communication about data [...] rarely comes in the form of raw data or code output. Instead, data and data-driven results are usually either summarized (e.g., using an average/mean) and presented in small summary tables or they are presented visually in the form of graphs, in which shape, distance, color, and size can be used to represent the magnitudes of (and relationships between) the values within our data.

 - *Viridical Data Science*, Yu and Barter

---

# Key Questions

* What question(s) am I trying to answer?
* What question(s) *could* this dataset answer?

---

## The Tidyverse Model

![](img/tidy_pipeline.png)

Loosely, EDA encompasses the Import -> Tidy -> Transform -> Visualize steps. Basically it is everything before we do modeling, prediction or inference.

EDA may involve some statistical summaries, but it does *not* include formal statistical analysis.

---

## EDA Checklist{.smaller}

The goal of EDA is to better understand your data. Let's use the **checklist**:

1.  Read in the data
2.  Check the size of the data
3.  Examine the variables and their types
4.  Look at the top and bottom of the data
5.  Visualize the distributions of key variables
6.  Check your expectations
7.  Validate with an external source
8.  Formulate a (simple) question
9.  Try the easy solution first
10.  Challenge your solution

(adapted from [Exploratory Data Analysis with R](https://bookdown.org/rdpeng/exdata/exploratory-data-analysis-checklist.html#follow-up-questions) by Roger D. Peng)

---

## Case study

We are going to use a dataset created from the National Center for Environmental Information (https://www.ncei.noaa.gov/). The data are 2019 hourly measurements from weather stations across the continental U.S.

![](img/weather.png)

---

## Formulate a Question

It is a good idea to first have a question such as:

  * Which weather stations reported the hottest and coldest daily temperatures? 
  * What day of the month was on average the hottest?
  * Is there correlation between temperature and humidity in my dataset?

---

## Read in the Data{.smaller}

There are several ways to read in data (some depend on the type of data you have):

* `read.table` or `read.csv` in base R for delimited files
* `readRDS` if you have a .rds dataset (this is a handy, compressed way of saving R objects)
* `read_csv`, `read_csv2`, `read_delim`, `read_fwf` from `library(readr)` that is part of the tidyverse
* `readxl()` from `library(readxl)` for .xls and .xlsx files
* `read_sas`, `read_spss`, `read_stata` from `library(haven)`
*  `fread` from `library(data.table)` for efficiently importing large datasets that are regular delimited files 

---

## Read in the Data{.smaller}

There are plenty of ways to do these tasks, but we will focus on base R. 

Since our data is stored as a (gzipped) CSV file, we could load it into R with `read.csv`, but we will use the more flexible `read.table`. I have it stored locally, but we will see how to load it straight from GitHub in the lab.

```{r readin-in}
met <- read.table('../data/met/met_all.gz',
                  header = TRUE, sep = ',')
```

We specify that the first line contains column names by setting `header = TRUE` and we indicate that commas are used to separate the different values (rather than tabs, spaces, etc.) by setting `sep = ','`.

---

## Working with `data.frame`s{.smaller}

This gave as a `data.frame` object, which is a standard R format for cleaned, rectangular data. Each row represents an observation and each column represents a variable.

As we have seen, you can access particular parts of the `data.frame` by subsetting with the square brackets, `[,]`. For example, you can pull out the 2nd, 3rd, and 4th elements of the 1st column of our `met` dataset with `met[2:4, 1]`.

You can also pull out specific columns by name, using the `$` operator. Since the first column is called `USAFID`, we could access the same subset as above with `met$USAFID[2:4]` (notice that there is no comma here, because we have already subset down to a single variable).

To see the list of names for the dataset, you can use `names(met)` or `colnames(met)`. To see the top few rows of the dataset, use `head(met)`.

---

## Check the data

We should check the dimensions of the data set. This can be done several ways:

```{r check-dims}
dim(met)
nrow(met)
ncol(met)
```


---

## Check the data

* We see that there are 2,377,343 records of hourly temperature in August 2019 from all of the weather stations in the US. The data set has 30 variables. 
* We should also check the top and bottom of the dataset to check for any irregularities. Use `head(met)` and `tail(met)` for this.
* Next we can take a deeper dive into the contents of the data with `str()`

---

## Check variables

```{r dat-struct}
str(met)
```

---

## Check variables

* First, we see that `str()` gives us the class of the data, which in this case is a `data.frame`, as well as the dimensions of the data
* We also see the variable names and their type (integer, numeric, character, etc.) 
* We can identify major problems with the data at this stage (e.g. a variable that has all missing values)

---

## Check variables

We can get summary statistics on our `data.frame` using `summary()`.

```{r, out.height="50%", out.width="50%"}
summary(met[,8:13])
```

---

## Check variables more closely

We know that we are supposed to have hourly measurements of weather data for the month of August 2019 for the entire US. We should check that we have all of these components. Let's check:

* the year
* the month
* the hours
* the range of locations (latitude and longitude)

---

## Check variables more closely

We can generate tables and/or barplots for integer variables:

```{r}
table(met$hour)
table(met$month)
```

## Check variables more closely

We can generate tables and/or barplots for integer variables:

```{r out.width = '50%', out.height='50%', fig.align = "center"}
barplot(table(met$hour))
```

---

## Check variables more closely

For numeric variables we should do a summary to see the quantiles, min, max, and mean.

```{r}
table(met$year)
summary(met$lat)
summary(met$lon)
```

---

## Check variables more closely{.smaller}

We can visualize these distributions with a histogram.

```{r out.width = '50%', out.height='50%', fig.align = "center"}
layout(matrix(1:2, nrow=1))
hist(met$lat)
hist(met$lon)
layout(1)
```

---

## Check variables more closely{.smaller}

If we return to our initial question, what weather stations reported the hottest and coldest temperatures, we should take a closer look at our key variable, temperature (`temp`)

```{r}
summary(met$temp)
hist(met$temp)
```

It looks like the temperatures are in Celsius. A temperature of -40 in August is really cold, we should see if this is an implausible value.

---

## Check variables more closely

It also looks like there is a lot of missing data, encoded by `NA` values. Let's check the proportion of missingness by tallying up whether or not every temperature reading is an `NA`. This will give us a vector of `TRUE`/`FALSE` values and then we can take the `mean` (average), because R automatically interprets `TRUE` as `1` and `FALSE` as `0` for mathematical functions.

```{r}
mean(is.na(met$temp))
```

2.5% of the data are missing, which is not a huge amount.

---



## Check variables more closely{.smaller}

In our `data.frame` we can easily subset the data and select certain columns. Here, we select all observations with a temperature of -40C and a specific subset of the variables:

```{r}
met_ss <- met[met$temp == -40.00, c('hour','lat','lon','elev','wind.sp')]

dim(met_ss)
summary(met_ss)
```

---

## Check variables more closely{.smaller}

In `dplyr` we can do the same thing using  `filter` and `select`

```{r}
library(dplyr)
met_ss <- filter(met, temp == -40.00) |> 
  select(USAFID, day, hour, lat, lon, elev, wind.sp)

dim(met_ss)
summary(met_ss)
```

---

## Validate against an external source{.smaller}

We should check outside sources to make sure that our data makes sense. For example the observation with -40C is suspicious, so we should look up the location of the weather station.

Go to [Google maps](https://www.google.com/maps/) and enter the coordinates for the site with -40C (29.12, -89.55)

![](img/map.png)

It doesn't make much sense to have a -40C reading in the Gulf of Mexico off the coast of Louisiana!

---

## Data cleaning{.smaller}

If we return to our initial question ("Which weather stations reported the hottest and coldest daily temperatures?"), we need to generate a list of weather stations that are ordered from highest to lowest. We can then examine the top and bottom of this new dataset.

First let us remove the aberrant observations and then we'll sort by temperature.

```{r}
met <- met[met$temp > -40, ]
```

Notice that we do not create a new object, we just overwrite the `met` object. Once you're sure that you want to remove certain observations, this is a good way to avoid confusion (otherwise, it is easy to end up with multiple subsets of the data in your R environment with similar names like `met`, `met_ss`, `met_ss2`, `met_final`, `met_FINAL`, `met_FINAL_REAL`, etc.)

---

---

## Data cleaning

We will also remove any observations with missing temperature values (`NA`).

The `is.na()` function tells you whether or not a particular value is missing and the `!` operator takes the opposite of a `TRUE`/`FALSE` value, so in combination, they tell you which observations are not missing.

```{r}
met <- met[!is.na(met$temp), ]
```


# Sorting

Now, we can use the `order()` function to sort our dataset.

```{r}
met <- met[order(met$temp), ]
```

Again, we just replace the `met` object with this updated version, since we aren't actually losing any data, just changing the order.

---

## Highest and Lowest

```{r}
head(met)[,c(1,8:10,24)]
tail(met)[,c(1,8:10,24)]
```

---

## Summary statistics

The maximum hourly temperature is `r met$temp[nrow(met)]`C at site `r met$USAFID[nrow(met)]`, and the minimum hourly temperature is `r met$temp[1]`C at site `r met$USAFID[1]`.

---

## Summary statistics{.smaller}

We need to transform our data to answer our initial question. Let's find the **daily** mean, max, and min temperatures for each weather station in our `data.frame`. We can do this with the `summarize` function from the `dplyr` package. This package is part of the `tidyverse`, so the syntax is a bit different from what we've seen before.

```{r}
library(dplyr)
met_daily <- summarize(met,
                       temp = mean(temp),
                       lat = mean(lat),
                       lon = mean(lon),
                       elev = mean(elev),
                       .by = c(USAFID, day))
```

What we've done here is told R to summarize the `met` dataset by the variables `USAFID` and `day`, splitting the data into subsets based on those two indexing variables. For each subset (representing a specific station of a specific day), we want the daily average temperature, as well as latitude, longitude, and elevation (though hopefully those don't change too much over the course of a day!)

---

## Summary statistics

Before we continue, check the relative sizes of the `met` and `met_daily` objects. Which one is bigger?

---

## Summary statistics{.smaller}

Now we will order our new dataset by the average daily temperature, just as we ordered the old one by observed temperature.

```{r}
met_daily <- met_daily[order(met_daily$temp), ]

head(met_daily)
tail(met_daily)
```

---

## Summary statistics

The maximum **daily average** temperature is `r met_daily$temp[nrow(met_daily)]` C at site `r met_daily$USAFID[nrow(met_daily)]` and the minimum daily average temperature is `r met_daily$temp[1]`C at site `r met_daily$USAFID[1]`.

---

## Summary statistics

The code below is similar to our previous example, but doesn't include the latitude, longitude, and elevation. How would you alter this code to find the daily **median**, **max**, or **min** temperatures for each station?

```{r, eval=FALSE}
summarize(met,
          temp = mean(temp),
          .by = c(USAFID, day))
```

(try it yourself)

---

## Exploratory graphs

With exploratory graphs we aim to:

* debug any issues remaining in the data
* understand properties of the data
* look for patterns in the data
* inform modeling strategies

Exploratory graphs do not need to be perfect, we will look at presentation ready plots next week.

---

## Exploratory graphs

Examples of exploratory graphs include:

* histograms
* boxplots
* scatterplots
* simple maps

---

## Exploratory Graphs

Focusing on the variable of interest, temperature, let's look at the distribution (after removing -40C)

```{r out.width = '50%', out.height='50%', fig.align = "center"}
hist(met$temp)
```

---

## Exploratory Graphs

Let's look at the daily data

```{r out.width = '50%', out.height='50%', fig.align = "center"}
hist(met_daily$temp)
```

---

## Exploratory Graphs

A boxplot gives us an idea of the quantiles of the distribution and any outliers

```{r out.width = '60%', out.height='60%', fig.align = "center"}
boxplot(met$temp, col = "blue")
```

---

## Exploratory Graphs

Let's look at the daily data

```{r out.width = '60%', out.height='60%', fig.align = "center"}
boxplot(met_daily$temp, col = "blue")
```

---

## Exploratory Graphs{.smaller}

We know that these data come from US weather stations, so we might have some idea what to expect just from plotting the latitude and longitude (note that we fix the aspect ratio at 1:1 with `asp = 1`; this prevents the plot from stretching or shrinking to fit the available plotting area):

```{r out.width = '60%', out.height='60%', fig.align = "center"}
plot(met_daily$lon, met_daily$lat, asp=1)
```

---

## Exploratory Graphs

A map will show us where the weather stations are located. First let's get the unique latitudes and longitudes and see how many meteorological sites there are

```{r}
met_stations <- (unique(met[,c("lat","lon")]))  
dim(met_stations)
```

---

## Exploratory Graphs

A map will show us where the weather stations are located. First let's get the unique latitudes and longitudes and see how many meteorological sites there are.

```{r eval=FALSE}
library(leaflet)
leaflet(met_stations) |> 
  addProviderTiles('CartoDB.Positron') |> 
  addCircles(lat = ~lat, lng = ~lon,
             opacity = 1, fillOpacity = 1, radius = 400)
```

---

## Exploratory Graphs

```{r echo=FALSE}
library(leaflet)
leaflet(met_stations) |> 
  addProviderTiles('CartoDB.Positron') |> 
  addCircles(lat = ~lat, lng = ~lon,
             opacity = 1, fillOpacity = 1, radius = 400)
```

---

## Exploratory Graphs

Let's map the locations of the max and min daily temperatures.

```{r eval=FALSE}
min <- met_daily[1, ]               # First observation
max <- met_daily[nrow(met_daily), ] # Last observation

leaflet() |> 
  addProviderTiles('CartoDB.Positron') |> 
  addCircles(
    data = min,
    lat = ~lat, lng = ~lon, popup = "Min temp.",
    opacity = 1, fillOpacity = 1, radius = 400, color = "blue"
    ) |>
  addCircles(
    data = max,
    lat = ~lat, lng = ~lon, popup = "Max temp.",
    opacity=1, fillOpacity=1, radius = 400, color = "red"
    )
```

(next slide)

---

```{r echo = FALSE}
min <- met_daily[1, ]  # First observation.
max <- met_daily[nrow(met_daily), ] # Last observation

leaflet() |> 
  addProviderTiles('CartoDB.Positron') |> 
  addCircles(
    data = min,
    lat = ~lat, lng = ~lon, popup = "Min temp.",
    opacity = 1, fillOpacity = 1, radius = 400, color = "blue"
    ) |>
  addCircles(
    data = max,
    lat = ~lat, lng = ~lon, popup = "Max temp.",
    opacity=1, fillOpacity=1, radius = 400, color = "red"
    )
```

---

## Exploratory Graphs{.smaller}

Scatterplots help us look at pairwise relationships. Let's see if there is any trend in temperature with latitude

```{r out.width = '40%', out.height='40%', fig.align = "center"}
plot(met_daily$lat, met_daily$temp, pch=16, cex=0.5)
```

There is a clear decrease in temperatures as you increase in latitude (i.e as you go north).

---

## Exploratory Graphs

We can add a simple linear regression line to this plot using `lm()` and `abline()`. We can also add a title and change the axis labels.

```{r eval=FALSE}
mod <- lm(temp ~ lat, data = met_daily)
plot(met_daily$lat, met_daily$temp, pch=19, cex=0.5, 
  main = "Temperature and Latitude", 
  xlab = "Latitude", ylab = "Temperature (deg C)")
abline(mod, lwd=2, col="red")
```

(next slide)

---

```{r out.width = '50%', out.height='40%', fig.align = "center", results='hide', echo=FALSE}
mod <- lm(temp ~ lat, data = met_daily)
plot(met_daily$lat, met_daily$temp,
     pch=19, cex=0.5, 
     main = "Temperature and Latitude", 
     xlab = "Latitude", ylab = "Temperature (deg C)")
abline(mod, lwd=2, col="red")
```

---

Using `ggplot2` (next class)

```{r last-ggplot2, out.width='50%'}
library(ggplot2)
ggplot(data = met_daily, mapping = aes(x = lat, y = temp)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Temperature and Latitute", xlab  = "Latitute", y = "Temperature (deg C)")
```

---

# Summary

In EDA we:

* have an initial question that we aim to answer
* import, check, clean the data
* perform any data transformations to answer the initial question
* make some basic graphs to examine the data and visualize the initial question

---

# Next topic (starting today): Tidyverse

---

## Tidyverse

```{r}
#| message: false

library(tidyverse)
```

* The _tidyverse_ is not a package but a group of packages developed to work with each other.


* The tidyverse makes data analysis simpler and code easier to read by sacrificing some flexibility. 


* One way code is simplified by ensuring all functions take and return _tidy data_.

--- 

## Tidy data 

* Stored in a data frame.

* Each observation is exactly one row.

* Variables are stored in columns.

* Not all data can be represented this way, but a very large subset of data analysis challenges are based on tidy data.

* Assuming data is tidy simplifies coding and frees up our minds for statistical thinking.

--- 


## Tidy data 

* This is an example of a tidy dataset:

```{r}
#| echo: false

library(dslabs)
tidy_data <- gapminder |> 
  filter(country %in% c("South Korea", "Germany") & !is.na(fertility)) |>
  select(country, year, fertility)
head(tidy_data, 6)
```

## Tidy data 

* Originally, the data was in the following format:

```{r, echo=FALSE, message=FALSE}
path <- system.file("extdata", package = "dslabs")
filename <- file.path(path, "fertility-two-countries-example.csv")
wide_data <- read_csv(filename)
select(wide_data, country, `1960`:`1970`) |> as.data.frame()
```

* This is **not** tidy.



## Tidyverse packages

* **tibble** - improves data frame class.

* **readr** - import data.

* **dplyr** - used to modify data frames.
    
* **ggplot2** - simplifies plotting.

* **tidyr** - helps convert data into tidy format.

* **stringr** - string processing.

* **forcats** - utilities for categorical data.

* **purrr** - tidy version of apply functions.

## dplyr

* In this lecture we focus mainly on **dplyr**.

* In particular the following functions:

    - `mutate` 
    
    - `select`
    
    - `across`
    
    - `filter`
    
    - `group_by`
    
    - `summarize`


--- 

## Adding a column with `mutate`


```{r, message=FALSE}
murders <- mutate(murders, rate = total/population*100000)
```

* Notice that here we used `total` and `population` inside the function, which are objects that are **not** defined in our workspace.

* This is known as non-standard evaluation where the context is used to know what variable names means.

* Tidyverse extensively uses non-standard evaluation.

* This can create confusion but it certainly simplifies code.

--- 

## Subsetting with `filter`

```{r}
filter(murders, rate <= 0.71)
```

--- 

## Selecting columns with `select`

```{r}
new_table <- select(murders, state, region, rate)
filter(new_table, rate <= 0.71)
```

--- 

## Transforming variables

* The function `mutate` can also be used to transform variables. 

* For example, the following code takes the log transformation of the population variable:

```{r}
#| eval: false
mutate(murders, population = log10(population))
```

--- 

## Transforming variables

* Often, we need to apply the same transformation to several variables. 

* The function `across` facilitates the operation. 

* For example if want to log transform both population and total murders we can use:

```{r}
#| eval: false
mutate(murders, across(c(population, total), log10))
```

--- 

## Transforming variables

* The helper functions come in handy when using across. 

* An example is if we want to apply the same transformation to all numeric variables:

```{r}
#| eval: false
mutate(murders, across(where(is.numeric), log10))
```

* or all character variables:

```{r}
#| eval: false
mutate(murders, across(where(is.character), tolower))
```

* There are several other useful helper functions.


--- 

## The pipe: `|>` or `%>%`

* We use the pipe to chain a series of operations.

* For example if we want to select columns and then filter rows we chain like this:

$$ \mbox{original data }
\rightarrow \mbox{ select }
\rightarrow \mbox{ filter } $$


--- 

## The pipe: `|>` or `%>%`

* The code looks like this:

```{r}
murders |> select(state, region, rate) |> filter(rate <= 0.71)
```

* The object on the left of the pipe is used as the first argument for the function on the right. 

* The second argument becomes the first, the third the second, and so on...

---


## The pipe: `|>` or `%>%`

* Here is a simple example:

```{r}
16 |> sqrt() |> log(base = 2)
```

---


## Summarizing data

* We use the **dplyr** `summarize` function, not to be confused with `summary` from R base.

* Here is an example of how it works:

```{r}
murders |> summarize(avg = mean(rate))
```

* Let's compute murder rate for the US. Is the above it?

--- 

## Summarizing data

* No, the rate is NOT the average of rates.

* It is the total murders divided by total population:

```{r}
murders |> summarize(rate = sum(total)/sum(population)*100000)
```

--- 

## Multiple summaries

* Suppose we want the median, minimum and max population size:

```{r}
murders |> summarize(median = median(population), min = min(population), max = max(population))
```

* Why don't we use `quantiles`? 

```{r}
murders |> summarize(quantiles = quantile(population, c(0.5, 0, 1)))
```


--- 

## Multiple summaries

:::{.callout-warning}
Using a function that returns more than one number within summarize will soon be deprecated.
:::

* For multiple summaries we use `reframe`:

```{r}
murders |> reframe(quantiles = quantile(population, c(0.5, 0, 1)))
```

--- 

## Multiple summaries

* However, if we want a column per summary, as when we called `min`, `median`, and `max` separately, 
we have to define a function that returns a data frame like this:

```{r}
median_min_max <- function(x){
  qs <- quantile(x, c(0.5, 0, 1))
  data.frame(median = qs[1], min = qs[2], max = qs[3])
}
```

* Then we can call `summarize`:

```{r}
murders |> summarize(median_min_max(population))
```

--- 

## Group then summarize 

* Let's compute murder rate by region.

* Take a close look at this output? 

```{r}
murders |> group_by(region) |> head(4)
```

* Note the `Groups: region [4]` at the top.

* This is a special data frame called a _grouped data frame_.

--- 

## Group then summarize  

* In particular `summarize`, will behave differently when acting on this object. 

```{r}
murders |> 
  group_by(region) |> 
  summarize(rate = sum(total) / sum(population) * 100000)
```

* The `summarize` function applies the summarization to each group separately.

--- 

## Group then summarize 

* For another example, let's compute the median, minimum, and maximum population in the four regions of the country using the `median_min_max` previously defined:


```{r}
murders |> group_by(region) |> summarize(median_min_max(population))
```

--- 

## Group then summarize  

* You can also summarize a variable but not collapse the dataset. 

* We use `mutate` instead of `summarize`. 

* Here is an example where we add a column with the population in each region and the number of states in the region, shown for each state. 

```{r}
#| eval: false
murders |> group_by(region) |> 
  mutate(region_pop = sum(population), n = n())
```

--- 

## ungroup

* When we do this, we usually want to ungroup before continuing our analysis. 

```{r}
#| eval: false
murders |> group_by(region) |> 
  mutate(region_pop = sum(population), n = n()) |>
  ungroup()
```

* This avoids having a grouped data frame that we don't need.
--- 

## `pull`


* Tidyverse function always returns a data frame. Even if its just one number.

```{r}
murders |> 
  summarize(rate = sum(total)/sum(population)*100000) |>
  class()
```

--- 

## `pull`

* To get a numeric use pull:

```{r}
murders |> 
  summarize(rate = sum(total)/sum(population)*100000) |>
  pull(rate) 
```

--- 

## Sorting data frames

* States order by rate

```{r}
murders |> arrange(rate) |> head()
```

--- 

## Sorting data frames

* If we want decreasing we can either use the negative or, for more readability, use `desc`:
```{r}
murders |> arrange(desc(rate)) |> head()
```

--- 

## Sorting data frames

* We can use two variables as well:

```{r}
murders |> arrange(region, desc(rate)) |> head(11)
```

